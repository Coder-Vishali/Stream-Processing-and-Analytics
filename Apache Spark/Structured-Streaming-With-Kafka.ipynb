{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 2.7.5\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************  SPARK Context Created!***********************\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('wiki-changes-event-consumer197')\n",
    "         # Add kafka package\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4\")\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\"********************  SPARK Context Created!***********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************  DF Loaded ***********************\n"
     ]
    }
   ],
   "source": [
    "# Create stream dataframe setting kafka server, topic and offset option\n",
    "df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \n",
    "  .option(\"subscribe\", \"wiki-changes\") \n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .load())\n",
    "  \n",
    "print(\"********************  DF Loaded ***********************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Convert binary to string key and value\n",
    "df1 = (df\n",
    "    .withColumn(\"key\", df[\"key\"].cast(StringType()))\n",
    "    .withColumn(\"value\", df[\"value\"].cast(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['key', 'value', 'topic', 'partition', 'offset', 'timestamp', 'timestampType']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, LongType, IntegerType\n",
    "\n",
    "# Event data schema\n",
    "schema_wiki = StructType(\n",
    "    [StructField(\"$schema\",StringType(),True),\n",
    "     StructField(\"bot\",BooleanType(),True),\n",
    "     StructField(\"comment\",StringType(),True),\n",
    "     StructField(\"id\",StringType(),True),\n",
    "     StructField(\"length\",\n",
    "                 StructType(\n",
    "                     [StructField(\"new\",IntegerType(),True),\n",
    "                      StructField(\"old\",IntegerType(),True)]),True),\n",
    "     StructField(\"meta\",\n",
    "                 StructType(\n",
    "                     [StructField(\"domain\",StringType(),True),\n",
    "                      StructField(\"dt\",StringType(),True),\n",
    "                      StructField(\"id\",StringType(),True),\n",
    "                      StructField(\"offset\",LongType(),True),\n",
    "                      StructField(\"partition\",LongType(),True),\n",
    "                      StructField(\"request_id\",StringType(),True),\n",
    "                      StructField(\"stream\",StringType(),True),\n",
    "                      StructField(\"topic\",StringType(),True),\n",
    "                      StructField(\"uri\",StringType(),True)]),True),\n",
    "     StructField(\"minor\",BooleanType(),True),\n",
    "     StructField(\"namespace\",IntegerType(),True),\n",
    "     StructField(\"parsedcomment\",StringType(),True),\n",
    "     StructField(\"patrolled\",BooleanType(),True),\n",
    "     StructField(\"revision\",\n",
    "                 StructType(\n",
    "                     [StructField(\"new\",IntegerType(),True),\n",
    "                      StructField(\"old\",IntegerType(),True)]),True),\n",
    "     StructField(\"server_name\",StringType(),True),\n",
    "     StructField(\"server_script_path\",StringType(),True),\n",
    "     StructField(\"server_url\",StringType(),True),\n",
    "     StructField(\"timestamp\",StringType(),True),\n",
    "     StructField(\"title\",StringType(),True),\n",
    "     StructField(\"type\",StringType(),True),\n",
    "     StructField(\"user\",StringType(),True),\n",
    "     StructField(\"wiki\",StringType(),True)])\n",
    "\n",
    "# Create dataframe setting schema for event data\n",
    "df_wiki = (df1\n",
    "           # Sets schema for event data\n",
    "           .withColumn(\"value\", from_json(\"value\", schema_wiki))\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['key', 'value', 'topic', 'partition', 'offset', 'timestamp', 'timestampType']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_unixtime, to_date, to_timestamp\n",
    "\n",
    "# Transform into tabular \n",
    "# Convert unix timestamp to timestamp\n",
    "# Create partition column (change_timestamp_date)\n",
    "df_wiki_formatted = (df_wiki.select(\n",
    "    col(\"key\").alias(\"event_key\")\n",
    "    ,col(\"topic\").alias(\"event_topic\")\n",
    "    ,col(\"timestamp\").alias(\"event_timestamp\")\n",
    "    ,col(\"value.$schema\").alias(\"schema\")\n",
    "    ,\"value.bot\"\n",
    "    ,\"value.comment\"\n",
    "    ,\"value.id\"\n",
    "    ,col(\"value.length.new\").alias(\"length_new\")\n",
    "    ,col(\"value.length.old\").alias(\"length_old\")\n",
    "    ,\"value.minor\"\n",
    "    ,\"value.namespace\"\n",
    "    ,\"value.parsedcomment\"\n",
    "    ,\"value.patrolled\"\n",
    "    ,col(\"value.revision.new\").alias(\"revision_new\")\n",
    "    ,col(\"value.revision.old\").alias(\"revision_old\")\n",
    "    ,\"value.server_name\"\n",
    "    ,\"value.server_script_path\"\n",
    "    ,\"value.server_url\"\n",
    "    ,to_timestamp(from_unixtime(col(\"value.timestamp\"))).alias(\"change_timestamp\")\n",
    "    ,to_date(from_unixtime(col(\"value.timestamp\"))).alias(\"change_timestamp_date\")\n",
    "    ,\"value.title\"\n",
    "    ,\"value.type\"\n",
    "    ,\"value.user\"\n",
    "    ,\"value.wiki\"\n",
    "    ,col(\"value.meta.domain\").alias(\"meta_domain\")\n",
    "    ,col(\"value.meta.dt\").alias(\"meta_dt\")\n",
    "    ,col(\"value.meta.id\").alias(\"meta_id\")\n",
    "    ,col(\"value.meta.offset\").alias(\"meta_offset\")\n",
    "    ,col(\"value.meta.partition\").alias(\"meta_partition\")\n",
    "    ,col(\"value.meta.request_id\").alias(\"meta_request_id\")\n",
    "    ,col(\"value.meta.stream\").alias(\"meta_stream\")\n",
    "    ,col(\"value.meta.topic\").alias(\"meta_topic\")\n",
    "    ,col(\"value.meta.uri\").alias(\"meta_uri\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[event_key: string, event_topic: string, event_timestamp: timestamp, schema: string, bot: boolean, comment: string, id: string, length_new: int, length_old: int, minor: boolean, namespace: int, parsedcomment: string, patrolled: boolean, revision_new: int, revision_old: int, server_name: string, server_script_path: string, server_url: string, change_timestamp: timestamp, change_timestamp_date: date, title: string, type: string, user: string, wiki: string, meta_domain: string, meta_dt: string, meta_id: string, meta_offset: bigint, meta_partition: bigint, meta_request_id: string, meta_stream: string, meta_topic: string, meta_uri: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# Same query as staticInputDF\n",
    "streamingCountsDF = (                 \n",
    "  df_wiki_formatted\n",
    "    .groupBy(\n",
    "      df_wiki_formatted.event_topic, \n",
    "      window(df_wiki_formatted.change_timestamp, \"1 second\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# Is this DF actually a streaming DF?\n",
    "streamingCountsDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# Same query as staticInputDF\n",
    "streamingCountsDF = (                 \n",
    "  df_wiki_formatted\n",
    "    .groupBy(\n",
    "      df_wiki_formatted.server_name, \n",
    "      window(df_wiki_formatted.event_timestamp, \"1 minute\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# Is this DF actually a streaming DF?\n",
    "streamingCountsDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n",
    "\n",
    "query = (\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")        # memory = store in-memory table \n",
    "    .queryName(\"counts\")     # counts = name of the in-memory table\n",
    "    .outputMode(\"complete\")  # complete = all the counts should be in the table\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spark.streams.active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:5\n",
      "+--------------------+--------------------+-----+\n",
      "|         server_name|              window|count|\n",
      "+--------------------+--------------------+-----+\n",
      "|    id.wikipedia.org|[1970-01-01 05:29...|   30|\n",
      "|    vi.wikipedia.org|[2021-09-03 16:05...|   25|\n",
      "|    th.wikipedia.org|[2021-09-03 16:08...|    3|\n",
      "|   ca.wiktionary.org|[2021-09-03 16:12...|   12|\n",
      "|    ar.wikipedia.org|[2021-09-03 16:12...|   21|\n",
      "|    te.wikipedia.org|[2021-09-03 16:09...|    2|\n",
      "|zh-classical.wiki...|[2021-09-03 16:04...|    2|\n",
      "|    zh.wikipedia.org|[2021-09-03 16:08...|   26|\n",
      "|    nl.wikipedia.org|[2021-09-03 16:08...|    8|\n",
      "|   tr.wikisource.org|[2021-09-03 16:06...|    1|\n",
      "|   tr.wikisource.org|[2021-09-03 16:11...|    1|\n",
      "|    ta.wikipedia.org|[2021-09-03 16:12...|    2|\n",
      "|    ur.wikipedia.org|[2021-09-03 16:11...|    1|\n",
      "|    sk.wikipedia.org|[2021-09-03 16:12...|    4|\n",
      "|     ru.wikinews.org|[2021-09-03 16:02...|   40|\n",
      "|   tr.wikisource.org|[2021-09-03 16:02...|    1|\n",
      "|   en.wiktionary.org|[2021-09-03 16:07...|    4|\n",
      "|    ml.wikipedia.org|[2021-09-03 16:07...|    1|\n",
      "|    no.wikipedia.org|[2021-09-03 16:01...|    1|\n",
      "|species.wikimedia...|[2021-09-03 16:09...|    1|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'queryStreamMem' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5c8d2031c6b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Stop Query Stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mqueryStreamMem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stream process interrupted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'queryStreamMem' is not defined"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Count rows every 5 seconds while stream is active\n",
    "try:\n",
    "    i=1\n",
    "    # While stream is active, print count\n",
    "    while len(spark.streams.active) > 0:\n",
    "        \n",
    "        # Clear output\n",
    "        clear_output(wait=True)\n",
    "        print(\"Run:{}\".format(i))\n",
    "        \n",
    "        lst_queries = []\n",
    "        for s in spark.streams.active:\n",
    "            lst_queries.append(s.name)\n",
    "\n",
    "        # Verify if wiki_changes_count query is active before count\n",
    "        if \"counts\" in lst_queries:\n",
    "            # Count number of events\n",
    "            spark.sql(\"select * from counts\").show()\n",
    "        else:\n",
    "            print(\"'wiki_changes_count' query not found.\")\n",
    "\n",
    "        sleep(5)\n",
    "        i=i+1\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    # Stop Query Stream\n",
    "    queryStreamMem.stop()\n",
    "    \n",
    "    print(\"stream process interrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
